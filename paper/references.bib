@article{hatefi2025sparc3,
  title={Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in {LLM}s},
  author={Hatefi, Sayed Mohammad Vakilzadeh and Dreyer, Maximilian and Achtibat, Reduan and Kahardipraja, Patrick and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
  journal={arXiv preprint arXiv:2506.13727},
  year={2025}
}

@inproceedings{achtibat2024attnlrp,
  title={{AttnLRP}: Attention-aware layer-wise relevance propagation for transformers},
  author={Achtibat, Reduan and Hatefi, Sayed Mohammad Vakilzadeh and Dreyer, Maximilian and Jain, Arjun and Wiegand, Thomas and Lapuschkin, Sebastian and Samek, Wojciech},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={135--168},
  year={2024},
  organization={PMLR}
}

@article{bach2015lrp,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015}
}

@incollection{montavon2019lrp,
  title={Layer-Wise Relevance Propagation: An Overview},
  author={Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  booktitle={Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  pages={193--209},
  year={2019},
  publisher={Springer}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={{RealToxicityPrompts}: Evaluating Neural Toxic Degeneration in Language Models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={3356--3369},
  year={2020}
}

@inproceedings{nadeem2021stereoset,
  title={{StereoSet}: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages={5356--5371},
  year={2021}
}

@inproceedings{nangia2020crows,
  title={{CrowS-Pairs}: A Challenge Dataset for Measuring Social Biases in Masked Language Models},
  author={Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages={1953--1967},
  year={2020}
}

@inproceedings{dhamala2021bold,
  title={{BOLD}: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={862--872},
  year={2021}
}

@inproceedings{parrish2022bbq,
  title={{BBQ}: A Hand-Built Bias Benchmark for Question Answering},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={2086--2105},
  year={2022}
}

@misc{wu2023mgs,
  title={Multi-Grain Stereotype ({MGS}) Dataset for Bias Evaluation},
  author={Wu, Zhijing and others},
  year={2023},
  note={Stereotype-Elicitation-Prompt-Library}
}

@article{raffel2020c4,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{merity2016wikitext,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@misc{meta2024llama3,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  howpublished={\url{https://llama.meta.com/llama3}}
}

@inproceedings{zhang2022opt,
  title={{OPT}: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  booktitle={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@misc{detoxify,
  title={Detoxify},
  author={Hanu, Laura and {Unitary team}},
  year={2020},
  howpublished={\url{https://github.com/unitaryai/detoxify}}
}

@inproceedings{conmy2023acdc,
  title={Towards Automated Circuit Discovery for Mechanistic Interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16318--16352},
  year={2023}
}

@article{ferrando2024ifr,
  title={Information Flow Routes: Automatically Interpreting Language Models at Scale},
  author={Ferrando, Javier and Voita, Elena},
  journal={arXiv preprint arXiv:2403.00824},
  year={2024}
}

@inproceedings{voita2019heads,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5797--5808},
  year={2019}
}

@article{yeom2021pruning,
  title={Pruning by explaining: A novel criterion for deep neural network pruning},
  author={Yeom, Seong-Kyun and Seegerer, Philipp and Lapuschkin, Sebastian and Binder, Alexander and Wiedemann, Simon and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={Pattern Recognition},
  volume={115},
  pages={107899},
  year={2021}
}

@article{hatefi2024pruning,
  title={Pruning by Explaining Revisited: Optimizing Attribution Methods to Prune {CNN}s and Transformers},
  author={Hatefi, Sayed Mohammad Vakilzadeh and Dreyer, Maximilian and Achtibat, Reduan and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
  journal={arXiv preprint arXiv:2408.12568},
  year={2024}
}

@article{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{abid2021persistent,
  title={Persistent Anti-Muslim Bias in Large Language Models},
  author={Abid, Abubakar and Farooqi, Maheen and Zou, James},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={298--306},
  year={2021}
}

@inproceedings{zhao2018winobias,
  title={{WinoBias}: Detecting Gender Bias in Coreference Resolution},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={8--14},
  year={2018}
}

@article{elhage2021mathematical,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  year={2021},
  howpublished={\url{https://transformer-circuits.pub/2021/framework}}
}

@article{sun2023wanda,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

